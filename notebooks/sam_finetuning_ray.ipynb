{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'elf.io'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label \u001b[38;5;28;01mas\u001b[39;00m connected_components\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_em\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_loader\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_em\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinInstanceSampler\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_em\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_random_colors\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. include:: ../doc/start_page.md\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m.. include:: ../doc/datasets_and_dataloaders.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msegmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     default_segmentation_dataset,\n\u001b[1;32m      7\u001b[0m     default_segmentation_loader,\n\u001b[1;32m      8\u001b[0m     default_segmentation_trainer,\n\u001b[1;32m      9\u001b[0m     get_data_loader,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/segmentation.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConcatDataset, ImageCollectionDataset, SegmentationDataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiceLoss\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultTrainer\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/data/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcat_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConcatDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetWrapper\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_collection_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageCollectionDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mraw_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RawDataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpseudo_label_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PseudoLabelDataset\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/data/image_collection_dataset.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     ensure_spatial_array, ensure_tensor_with_channels, load_image, supports_memmap, ensure_patch_shape\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImageCollectionDataset\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m     13\u001b[0m     max_sampling_attempts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/util/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data, load_image, supports_memmap\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreporting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_training_summary\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parser_helper\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/util/image.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_file\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'elf.io'"
     ]
    }
   ],
   "source": [
    "# Check if we are running this notebook on kaggle, google colab or local compute resources.\n",
    "import os\n",
    "import ray\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"once\")\n",
    "\n",
    "from glob import glob\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import label as connected_components\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.data import MinInstanceSampler\n",
    "from torch_em.util.util import get_random_colors\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.append('/home/scheng/workspace/RaySam/micro-sam-ray')\n",
    "import micro_sam_ray.training as sam_training\n",
    "from micro_sam_ray.sample_data import (\n",
    "    fetch_tracking_example_data,\n",
    "    fetch_tracking_segmentation_data,\n",
    ")\n",
    "from micro_sam_ray.automatic_segmentation import (\n",
    "    get_predictor_and_segmenter,\n",
    "    automatic_instance_segmentation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automatic_instance_segmentation(\n",
    "    image, checkpoint_path, model_type=\"vit_b_lm\", device=None\n",
    "):\n",
    "    \"\"\"Automatic Instance Segmentation (AIS) by training an additional instance decoder in SAM.\n",
    "\n",
    "    NOTE: AIS is supported only for `µsam` models.\n",
    "\n",
    "    Args:\n",
    "        image: The input image.\n",
    "        checkpoint_path: The path to stored checkpoints.\n",
    "        model_type: The choice of the `µsam` model.\n",
    "        device: The device to run the model inference.\n",
    "\n",
    "    Returns:\n",
    "        The instance segmentation.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the 'predictor' and 'segmenter' to perform automatic instance segmentation.\n",
    "    predictor, segmenter = get_predictor_and_segmenter(\n",
    "        model_type=model_type,  # choice of the Segment Anything model\n",
    "        checkpoint=checkpoint_path,  # overwrite to pass your own finetuned model.\n",
    "        device=device,  # the device to run the model inference.\n",
    "    )\n",
    "\n",
    "    # Step 2: Get the instance segmentation for the given image.\n",
    "    prediction = automatic_instance_segmentation(\n",
    "        predictor=predictor,  # the predictor for the Segment Anything model.\n",
    "        segmenter=segmenter,  # the segmenter class responsible for generating predictions.\n",
    "        input_path=image,\n",
    "        ndim=2,\n",
    "    )\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def debug_notebook(training=True, inference=False):\n",
    "    root_dir = sys.path.append(os.getcwd())  # overwrite to set the root directory, where the data, checkpoints, and all relevant stuff will be stored\n",
    "\n",
    "    DATA_FOLDER = os.path.join(root_dir, \"data\")\n",
    "    os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "    # This will download the image and segmentation data for training.\n",
    "    image_dir = fetch_tracking_example_data(DATA_FOLDER)\n",
    "    segmentation_dir = fetch_tracking_segmentation_data(DATA_FOLDER)\n",
    "\n",
    "    image_paths = sorted(glob(os.path.join(image_dir, \"*\")))\n",
    "    segmentation_paths = sorted(glob(os.path.join(segmentation_dir, \"*\")))\n",
    "\n",
    "    # Load images from multiple files in folder via pattern (here: all tif files)\n",
    "    raw_key, label_key = \"*.tif\", \"*.tif\"\n",
    "\n",
    "    # Alternative: if you have tif stacks you can just set 'raw_key' and 'label_key' to None\n",
    "    # raw_key, label_key= None, None\n",
    "\n",
    "    # The 'roi' argument can be used to subselect parts of the data.\n",
    "    # Here, we use it to select the first 70 images (frames) for the train split and the other frames for the val split.\n",
    "    train_roi = np.s_[:70, :, :]\n",
    "    val_roi = np.s_[70:, :, :]\n",
    "\n",
    "    batch_size = 1  # the training batch size\n",
    "    patch_shape = (1, 512, 512)  # the size of patches for training\n",
    "\n",
    "    train_instance_segmentation = True\n",
    "\n",
    "    # There are cases where our inputs are large and the labeled objects are not evenly distributed across the image.\n",
    "    # For this we use samplers, which ensure that valid inputs are chosen subjected to the paired labels.\n",
    "    # The sampler chosen below makes sure that the chosen inputs have atleast one foreground instance, and filters out small objects.\n",
    "    sampler = MinInstanceSampler(\n",
    "        min_size=25\n",
    "    )  # NOTE: The choice of 'min_size' value is paired with the same value in 'min_size' filter in 'label_transform'.\n",
    "\n",
    "    # Update the train_loader and val_loader creation to use DistributedSampler\n",
    "    train_loader = sam_training.training_ray.default_sam_loader_distributed(\n",
    "        raw_paths=image_dir,\n",
    "        raw_key=raw_key,\n",
    "        label_paths=segmentation_dir,\n",
    "        label_key=label_key,\n",
    "        with_segmentation_decoder=train_instance_segmentation,\n",
    "        patch_shape=patch_shape,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    val_loader = sam_training.training_ray.default_sam_loader_distributed(\n",
    "        raw_paths=image_dir,\n",
    "        raw_key=raw_key,\n",
    "        label_paths=segmentation_dir,\n",
    "        label_key=label_key,\n",
    "        with_segmentation_decoder=train_instance_segmentation,\n",
    "        patch_shape=patch_shape,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    # NOTE (Jim): These loaders should be initialized in the train function, not here.\n",
    "\n",
    "    # All hyperparameters for training\n",
    "    n_objects_per_batch = 5  # the number of objects per batch that will be sampled\n",
    "    n_epochs = 5  # how long we train (in epochs)\n",
    "    model_type = \"vit_b\"  # using vit_b for faster training\n",
    "    checkpoint_name = \"sam_hela\"\n",
    "\n",
    "    if training:\n",
    "        # Initialize Ray if not already initialized\n",
    "        if not ray.is_initialized():\n",
    "            ray.init()\n",
    "\n",
    "        # Configure the scaling for distributed training\n",
    "        scaling_config = ScalingConfig(\n",
    "            num_workers=2,  # number of worker processes\n",
    "            use_gpu=True,  # use GPU\n",
    "            resources_per_worker={\n",
    "                \"CPU\": 8,  # limit CPU cores per worker\n",
    "                \"GPU\": 1,  # each worker gets 2 GPUs\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # NOTE: We should avoid passing the dataloaders to the training config, as suggested by ray.\n",
    "        train_config = {\n",
    "            \"name\": checkpoint_name,\n",
    "            \"save_root\": os.path.join(root_dir, \"saved_runs\"),\n",
    "            \"model_type\": model_type,\n",
    "            \"train_loader\": train_loader,\n",
    "            \"val_loader\": val_loader,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"lr\": 1e-5,\n",
    "            \"wd\": 1e-2,\n",
    "            \"n_objects_per_batch\": n_objects_per_batch,\n",
    "            \"with_segmentation_decoder\": train_instance_segmentation,\n",
    "            \"device\": \"ray\",\n",
    "        }\n",
    "\n",
    "        # Initialize the distributed trainer\n",
    "        trainer = TorchTrainer(\n",
    "            train_loop_per_worker=sam_training.train_sam_worker,\n",
    "            train_loop_config=train_config,\n",
    "            scaling_config=scaling_config,\n",
    "            run_config=ray.train.RunConfig(\n",
    "                storage_path=\"/storage/raysam_user/tmp/debug_sam_finetuning_ray\",\n",
    "                name=\"debug_sam_finetuning_ray\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Run distributed training\n",
    "        result = trainer.fit()\n",
    "        print(f\"Training completed with result: {result}\")\n",
    "\n",
    "    best_checkpoint = os.path.join(\n",
    "        root_dir, \"saved_runs\", \"checkpoints\", \"checkpoint_best.pt\"\n",
    "    )\n",
    "\n",
    "    if inference:\n",
    "        assert os.path.exists(\n",
    "            best_checkpoint\n",
    "        ), \"Please train the model first to run inference on the finetuned model.\"\n",
    "        assert (\n",
    "            train_instance_segmentation is True\n",
    "        ), \"Oops. You didn't opt for finetuning using the decoder-based automatic instance segmentation.\"\n",
    "\n",
    "        # Let's check the first 5 images. Feel free to comment out the line below to run inference on all images.\n",
    "        image_paths = image_paths[:5]\n",
    "\n",
    "        for idx, image_path in enumerate(image_paths):\n",
    "            image = imageio.imread(image_path)\n",
    "\n",
    "            # Predicted instances\n",
    "            prediction = run_automatic_instance_segmentation(\n",
    "                image=image,\n",
    "                checkpoint_path=best_checkpoint,\n",
    "                model_type=model_type,\n",
    "                device=\"cuda\",\n",
    "            )\n",
    "\n",
    "            # Visualize the predictions\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "            ax[0].imshow(image, cmap=\"gray\")\n",
    "            ax[0].axis(\"off\")\n",
    "            ax[0].set_title(\"Input Image\")\n",
    "\n",
    "            ax[1].imshow(\n",
    "                prediction, cmap=get_random_colors(prediction), interpolation=\"nearest\"\n",
    "            )\n",
    "            ax[1].axis(\"off\")\n",
    "            ax[1].set_title(\"Predictions (AIS)\")\n",
    "\n",
    "            plt.show()\n",
    "            # Save the figure\n",
    "            plt.savefig(\n",
    "                os.path.join(root_dir, \"saved_runs\", f\"checkpoint_best_img{idx}.png\")\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_notebook(training=True, inference=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raysam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
