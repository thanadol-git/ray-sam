{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42b464e",
   "metadata": {},
   "source": [
    "# Finetuning Segment Anything with `µsam`\n",
    "\n",
    "This notebook shows how to use Segment Anything for Microscopy to fine-tune a Segment Anything Model (SAM) on your custom data.\n",
    "\n",
    "We use DIC microscopy images from the Cell Tracking Challenge, DIC-C2DH-HeLa, HeLa cells on a flat glass (from [Maška et al.](https://www.nature.com/articles/s41592-023-01879-y)) in this notebook. The functionalities shown here should work for your (microscopy) images too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc7e43",
   "metadata": {},
   "source": [
    "## Running this notebook\n",
    "\n",
    "If you have an environment with `µsam` on your computer you can run this notebook in there. You can follow the [installation instructions](https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation) to install it on your computer.\n",
    "\n",
    "You can also run this notebook in the cloud on [Kaggle Notebooks](https://www.kaggle.com/code/). This service offers free usage of a GPU to speed up running the code. The next cells will take care of the installation for you if you are using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae4c578",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:07:06.368382Z",
     "iopub.status.busy": "2024-10-20T13:07:06.367697Z",
     "iopub.status.idle": "2024-10-20T13:07:06.381816Z",
     "shell.execute_reply": "2024-10-20T13:07:06.380756Z",
     "shell.execute_reply.started": "2024-10-20T13:07:06.368339Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/scheng/workspace/RaySam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scheng/miniconda3/envs/raysam/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-21 10:13:48,746\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-21 10:13:49,277\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-21 10:13:50,379\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using a behind-the-scenes resource. Follow our installation instructions here: https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation\n"
     ]
    }
   ],
   "source": [
    "# Check if we are running this notebook on kaggle, google colab or local compute resources.\n",
    "import os\n",
    "print(os.getcwd())\n",
    "import ray\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "current_spot = os.getcwd()\n",
    "\n",
    "if current_spot.startswith(\"/kaggle/working\"):\n",
    "    print(\"Kaggle says hi!\")\n",
    "    root_dir = \"/kaggle/working\"\n",
    "\n",
    "elif current_spot.startswith(\"/content\"):\n",
    "    print(\"Google Colab says hi!\")\n",
    "    print(\" NOTE: The scripts have not been tested on Google Colab, you might need to adapt the installations a bit.\")\n",
    "    root_dir = \"/content\"\n",
    "\n",
    "    # You might need to install condacolab on Google Colab to be able to install packages using conda / mamba\n",
    "    # !pip install -q condacolab\n",
    "    # import condacolab\n",
    "    # condacolab.install()\n",
    "\n",
    "else:\n",
    "    msg = \"You are using a behind-the-scenes resource. Follow our installation instructions here:\"\n",
    "    msg += \" https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation\"\n",
    "    print(msg)\n",
    "    root_dir = \"/storage/jingyug/GitHub/RaySam\"  # overwrite to set the root directory, where the data, checkpoints, and all relevant stuff will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db76f1",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The next cells will install the `micro_sam` library on Kaggle Notebooks. **Please skip these cells and go to `Importing the libraries` if you are running the notebook on your own computer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d9f1d6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:07:06.427894Z",
     "iopub.status.busy": "2024-10-20T13:07:06.426901Z",
     "iopub.status.idle": "2024-10-20T13:07:39.188750Z",
     "shell.execute_reply": "2024-10-20T13:07:39.187420Z",
     "shell.execute_reply.started": "2024-10-20T13:07:06.427850Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Install the newest version of micro-sam\n",
    "# current_spot = os.getcwd()\n",
    "\n",
    "# !git clone --quiet https://github.com/computational-cell-analytics/micro-sam.git\n",
    "# tmp_dir = os.path.join(current_spot, \"micro-sam\")\n",
    "# !pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcbe92b",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:07:39.191880Z",
     "iopub.status.busy": "2024-10-20T13:07:39.191438Z",
     "iopub.status.idle": "2024-10-20T13:07:56.757432Z",
     "shell.execute_reply": "2024-10-20T13:07:56.756050Z",
     "shell.execute_reply.started": "2024-10-20T13:07:39.191833Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !git clone --quiet https://github.com/constantinpape/torch-em.git\n",
    "# tmp_dir = os.path.join(root_dir, \"torch-em\")\n",
    "# !pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fbf988",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:07:56.759356Z",
     "iopub.status.busy": "2024-10-20T13:07:56.759000Z",
     "iopub.status.idle": "2024-10-20T13:08:14.881826Z",
     "shell.execute_reply": "2024-10-20T13:08:14.880532Z",
     "shell.execute_reply.started": "2024-10-20T13:07:56.759320Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !git clone --quiet https://github.com/constantinpape/elf.git\n",
    "# tmp_dir = os.path.join(root_dir, \"elf\")\n",
    "# !pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8788060f",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:08:14.884617Z",
     "iopub.status.busy": "2024-10-20T13:08:14.884261Z",
     "iopub.status.idle": "2024-10-20T13:09:34.264604Z",
     "shell.execute_reply": "2024-10-20T13:09:34.263391Z",
     "shell.execute_reply.started": "2024-10-20T13:08:14.884582Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !mamba install -q -y -c conda-forge nifty affogato zarr z5py natsort > /dev/null 2>&1\n",
    "# !pip uninstall -y --quiet qtpy  # qtpy is not supported in Kaggle / Google Colab, let's remove it to avoid errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81a915",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d5d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T13:09:34.266525Z",
     "iopub.status.busy": "2024-10-20T13:09:34.266204Z",
     "iopub.status.idle": "2024-10-20T13:09:56.762392Z",
     "shell.execute_reply": "2024-10-20T13:09:56.761535Z",
     "shell.execute_reply.started": "2024-10-20T13:09:34.266491Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_em.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label \u001b[38;5;28;01mas\u001b[39;00m connected_components\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_em\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_loader\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_em\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinInstanceSampler\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_em\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_random_colors\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. include:: ../doc/start_page.md\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m.. include:: ../doc/datasets_and_dataloaders.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msegmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     default_segmentation_dataset,\n\u001b[1;32m      7\u001b[0m     default_segmentation_loader,\n\u001b[1;32m      8\u001b[0m     default_segmentation_trainer,\n\u001b[1;32m      9\u001b[0m     get_data_loader,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/workspace/RaySam/torch_em/segmentation.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConcatDataset, ImageCollectionDataset, SegmentationDataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiceLoss\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultTrainer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_em.data'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from glob import glob\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import label as connected_components\n",
    "\n",
    "import torch\n",
    "import torch_em\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.data import MinInstanceSampler\n",
    "from torch_em.util.util import get_random_colors\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/home/scheng/workspace/RaySam/micro-sam-ray')\n",
    "import micro_sam_ray.training as sam_training\n",
    "from micro_sam_ray.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data\n",
    "from micro_sam_ray.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251c4bf",
   "metadata": {},
   "source": [
    "### Let's download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09048a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T13:09:56.765037Z",
     "iopub.status.busy": "2024-10-20T13:09:56.763826Z",
     "iopub.status.idle": "2024-10-20T13:10:08.514753Z",
     "shell.execute_reply": "2024-10-20T13:10:08.513665Z",
     "shell.execute_reply.started": "2024-10-20T13:09:56.764962Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(root_dir, \"data\")\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "# This will download the image and segmentation data for training.\n",
    "image_dir = fetch_tracking_example_data(DATA_FOLDER)\n",
    "segmentation_dir = fetch_tracking_segmentation_data(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f23c2",
   "metadata": {},
   "source": [
    "### Let's create the dataloaders\n",
    "\n",
    "Our task is to segment HeLa cells on a flat glass in DIC microscopic images. The dataset comes from https://celltrackingchallenge.net/2d-datasets/, and the dataloader has been implemented in [torch-em](https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/ctc.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc826a8",
   "metadata": {},
   "source": [
    "#### First, let's visualize how our samples look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18cc48",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:10:08.516821Z",
     "iopub.status.busy": "2024-10-20T13:10:08.516486Z",
     "iopub.status.idle": "2024-10-20T13:10:08.944358Z",
     "shell.execute_reply": "2024-10-20T13:10:08.943354Z",
     "shell.execute_reply.started": "2024-10-20T13:10:08.516786Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_paths = sorted(glob(os.path.join(image_dir, \"*\")))\n",
    "segmentation_paths = sorted(glob(os.path.join(segmentation_dir, \"*\")))\n",
    "\n",
    "for image_path, segmentation_path in zip(image_paths, segmentation_paths):\n",
    "    image = imageio.imread(image_path)\n",
    "    segmentation = imageio.imread(segmentation_path)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    segmentation = connected_components(segmentation)\n",
    "    ax[1].imshow(segmentation, cmap=get_random_colors(segmentation), interpolation=\"nearest\")\n",
    "    ax[1].set_title(\"Ground Truth Instances\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    break  # comment this out in case you want to visualize all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2ef3d",
   "metadata": {},
   "source": [
    "#### Next, let's create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bf611",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:10:08.946547Z",
     "iopub.status.busy": "2024-10-20T13:10:08.945690Z",
     "iopub.status.idle": "2024-10-20T13:10:08.952651Z",
     "shell.execute_reply": "2024-10-20T13:10:08.951589Z",
     "shell.execute_reply.started": "2024-10-20T13:10:08.946506Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 'micro_sam.training.default_sam_loader' is a convenience function to build a pytorch dataloader from image data and labels for training segmentation models.\n",
    "# This is wrapped around the 'torch_em.default_segmentation_loader'.\n",
    "# It supports image data in various formats.\n",
    "# Here, we load image data and labels from the two folders with tif images that were downloaded by the example data functionality,\n",
    "# by specifying `raw_key` and `label_key` as `*.tif`.\n",
    "# This means all images in the respective folders that end with .tif will be loaded.\n",
    "# The function supports many other file formats. For example, if you have tif stacks with multiple slices instead of multiple tif images in a folder,\n",
    "# then you can pass 'raw_key=label_key=None'.\n",
    "# For more information, here is the documentation: https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/README.md\n",
    "# And here is a tutorial on creating dataloaders using 'torch-em': https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\n",
    "\n",
    "# Load images from multiple files in folder via pattern (here: all tif files)\n",
    "raw_key, label_key = \"*.tif\", \"*.tif\"\n",
    "\n",
    "# Alternative: if you have tif stacks you can just set 'raw_key' and 'label_key' to None\n",
    "# raw_key, label_key= None, None\n",
    "\n",
    "# The 'roi' argument can be used to subselect parts of the data.\n",
    "# Here, we use it to select the first 70 images (frames) for the train split and the other frames for the val split.\n",
    "train_roi = np.s_[:70, :, :]\n",
    "val_roi = np.s_[70:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a565e47",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:10:08.954756Z",
     "iopub.status.busy": "2024-10-20T13:10:08.954376Z",
     "iopub.status.idle": "2024-10-20T13:10:09.434603Z",
     "shell.execute_reply": "2024-10-20T13:10:09.433584Z",
     "shell.execute_reply.started": "2024-10-20T13:10:08.954723Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The script below returns the train or val data loader for finetuning Segment Anything Model (SAM).\n",
    "# The data loader must be a torch data loader that returns `x, y` tensors, where `x` is the image data and `y` are the labels.\n",
    "# The labels have to be in a label mask instance segmentation format.\n",
    "# i.e. a tensor of the same spatial shape as `x`, with each object mask having its own ID.\n",
    "# Important: the ID 0 is reseved for background, and the IDs must be consecutive\n",
    "\n",
    "# Here, we use `micro_sam.training.default_sam_loader` for creating a suitable data loader from\n",
    "# the example hela data. You can either adapt this for your own data or write a suitable torch dataloader yourself.\n",
    "# Here's a quickstart notebook to create your own dataloaders: https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\n",
    "\n",
    "batch_size = 1  # the training batch size\n",
    "patch_shape = (1, 512, 512)  # the size of patches for training\n",
    "\n",
    "# Train an additional convolutional decoder for end-to-end automatic instance segmentation\n",
    "# NOTE 1: It's important to have densely annotated-labels while training the additional convolutional decoder.\n",
    "# NOTE 2: In case you do not have labeled images, we recommend using `micro-sam` annotator tools to annotate as many objects as possible per image for best performance.\n",
    "train_instance_segmentation = True\n",
    "\n",
    "# NOTE: The dataloader internally takes care of adding label transforms: i.e. used to convert the ground-truth\n",
    "# labels to the desired instances for finetuning Segment Anythhing, or, to learn the foreground and distances\n",
    "# to the object centers and object boundaries for automatic segmentation.\n",
    "\n",
    "# There are cases where our inputs are large and the labeled objects are not evenly distributed across the image.\n",
    "# For this we use samplers, which ensure that valid inputs are chosen subjected to the paired labels.\n",
    "# The sampler chosen below makes sure that the chosen inputs have atleast one foreground instance, and filters out small objects.\n",
    "sampler = MinInstanceSampler(min_size=25)  # NOTE: The choice of 'min_size' value is paired with the same value in 'min_size' filter in 'label_transform'.\n",
    "\n",
    "# Update the train_loader and val_loader creation to use DistributedSampler\n",
    "train_loader = sam_training.training_ray.default_sam_loader_distributed(\n",
    "  raw_paths=image_dir,\n",
    "  raw_key=raw_key,\n",
    "  label_paths=segmentation_dir,\n",
    "  label_key=label_key,\n",
    "  with_segmentation_decoder=train_instance_segmentation,\n",
    "  patch_shape=patch_shape,\n",
    "  batch_size=batch_size,\n",
    "  sampler=sampler,\n",
    ")\n",
    "\n",
    "val_loader = sam_training.training_ray.default_sam_loader_distributed(\n",
    "  raw_paths=image_dir,\n",
    "  raw_key=raw_key,\n",
    "  label_paths=segmentation_dir,\n",
    "  label_key=label_key,\n",
    "  with_segmentation_decoder=train_instance_segmentation,\n",
    "  patch_shape=patch_shape,\n",
    "  batch_size=batch_size,\n",
    "  sampler=sampler,\n",
    ")\n",
    "\n",
    "# NOTE (Jim): These loaders should be initialized in the train function, not here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd97f3",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:10:09.437894Z",
     "iopub.status.busy": "2024-10-20T13:10:09.437582Z",
     "iopub.status.idle": "2024-10-20T13:10:13.640753Z",
     "shell.execute_reply": "2024-10-20T13:10:13.639724Z",
     "shell.execute_reply.started": "2024-10-20T13:10:09.437860Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's check how our samples look from the dataloader\n",
    "check_loader(train_loader, 4, plt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137f18f",
   "metadata": {},
   "source": [
    "### Run the actual model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95602b6",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:10:13.642371Z",
     "iopub.status.busy": "2024-10-20T13:10:13.642059Z",
     "iopub.status.idle": "2024-10-20T13:10:13.647753Z",
     "shell.execute_reply": "2024-10-20T13:10:13.646862Z",
     "shell.execute_reply.started": "2024-10-20T13:10:13.642337Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Ray if not already initialized\n",
    "if not ray.is_initialized():\n",
    "  ray.init()\n",
    "\n",
    "# All hyperparameters for training\n",
    "n_objects_per_batch = 5  # the number of objects per batch that will be sampled\n",
    "n_epochs = 10  # how long we train (in epochs)\n",
    "model_type = \"vit_b\"  # using vit_b for faster training\n",
    "checkpoint_name = \"sam_hela\"\n",
    "\n",
    "# Configure the scaling for distributed training\n",
    "scaling_config = ScalingConfig(\n",
    "  num_workers=2,  # number of worker processes\n",
    "  use_gpu=True,  # use GPU\n",
    "  resources_per_worker={\n",
    "      \"CPU\": 8,  # limit CPU cores per worker\n",
    "      \"GPU\": 1  # each worker gets half a GPU\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733d170",
   "metadata": {},
   "source": [
    "**NOTE**: The user needs to decide whether to finetune the Segment Anything model, or the `µsam`'s \"finetuned microscopy models\" for their dataset. Here, we finetune on the Segment Anything model for simplicity. For example, if you choose to finetune the model from the light microscopy generalist models, you need to update the `model_type` to `vit_b_lm` and it takes care of initializing the model with the desired weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb463ac2",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:10:13.649350Z",
     "iopub.status.busy": "2024-10-20T13:10:13.648942Z",
     "iopub.status.idle": "2024-10-20T13:37:26.431919Z",
     "shell.execute_reply": "2024-10-20T13:37:26.430978Z",
     "shell.execute_reply.started": "2024-10-20T13:10:13.649316Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: We should avoid passing the dataloaders to the training config, as suggested by ray.\n",
    "train_config = {\n",
    "    \"name\": checkpoint_name,\n",
    "    \"save_root\": os.path.join(root_dir, \"saved_runs\"),\n",
    "    \"model_type\": model_type,\n",
    "    \"train_loader\": train_loader,\n",
    "    \"val_loader\": val_loader,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"n_objects_per_batch\": n_objects_per_batch,\n",
    "    \"with_segmentation_decoder\": train_instance_segmentation,\n",
    "    \"device\": \"ray\",\n",
    "}\n",
    "\n",
    "# Initialize the distributed trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=sam_training.train_sam_worker,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=\"/storage/raysam_user/tmp/debug_sam_finetuning_ray\",\n",
    "        name=\"debug_sam_finetuning_ray\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run distributed training\n",
    "result = trainer.fit()\n",
    "print(f\"Training completed with result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beea8c2",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:37:26.434283Z",
     "iopub.status.busy": "2024-10-20T13:37:26.433330Z",
     "iopub.status.idle": "2024-10-20T13:37:26.442427Z",
     "shell.execute_reply": "2024-10-20T13:37:26.441379Z",
     "shell.execute_reply.started": "2024-10-20T13:37:26.434240Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_checkpoint = os.path.join(root_dir, \"saved_runs\", \"checkpoints\", f\"checkpoint_epoch_{n_epochs}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0413028",
   "metadata": {},
   "source": [
    "### Let's run the automatic instance segmentation (AIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c2d04",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:38:37.638078Z",
     "iopub.status.busy": "2024-10-20T13:38:37.637246Z",
     "iopub.status.idle": "2024-10-20T13:38:37.644292Z",
     "shell.execute_reply": "2024-10-20T13:38:37.643399Z",
     "shell.execute_reply.started": "2024-10-20T13:38:37.638035Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_automatic_instance_segmentation(image, checkpoint_path, model_type=\"vit_b_lm\", device=None):\n",
    "    \"\"\"Automatic Instance Segmentation (AIS) by training an additional instance decoder in SAM.\n",
    "\n",
    "    NOTE: AIS is supported only for `µsam` models.\n",
    "\n",
    "    Args:\n",
    "        image: The input image.\n",
    "        checkpoint_path: The path to stored checkpoints.\n",
    "        model_type: The choice of the `µsam` model.\n",
    "        device: The device to run the model inference.\n",
    "\n",
    "    Returns:\n",
    "        The instance segmentation.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the 'predictor' and 'segmenter' to perform automatic instance segmentation.\n",
    "    predictor, segmenter = get_predictor_and_segmenter(\n",
    "        model_type=model_type, # choice of the Segment Anything model\n",
    "        checkpoint=checkpoint_path,  # overwrite to pass your own finetuned model.\n",
    "        device=device,  # the device to run the model inference.\n",
    "    )\n",
    "\n",
    "    # Step 2: Get the instance segmentation for the given image.\n",
    "    prediction = automatic_instance_segmentation(\n",
    "        predictor=predictor,  # the predictor for the Segment Anything model.\n",
    "        segmenter=segmenter,  # the segmenter class responsible for generating predictions.\n",
    "        input_path=image,\n",
    "        ndim=2,\n",
    "    )\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496c6e6",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:37:26.461869Z",
     "iopub.status.busy": "2024-10-20T13:37:26.461570Z",
     "iopub.status.idle": "2024-10-20T13:37:35.538080Z",
     "shell.execute_reply": "2024-10-20T13:37:35.536727Z",
     "shell.execute_reply.started": "2024-10-20T13:37:26.461827Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "zip_path = os.path.join(root_dir, \"data\", \"DIC-C2DH-HeLa-test.zip\")\n",
    "!wget -q http://data.celltrackingchallenge.net/test-datasets/DIC-C2DH-HeLa.zip -O $zip_path\n",
    "\n",
    "trg_dir = os.path.join(root_dir, \"data\", \"test\")\n",
    "print(trg_dir)\n",
    "os.makedirs(trg_dir, exist_ok=True)\n",
    "!unzip -q $zip_path -d trg_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca02e1",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-20T13:38:40.526159Z",
     "iopub.status.busy": "2024-10-20T13:38:40.525511Z",
     "iopub.status.idle": "2024-10-20T13:38:56.818736Z",
     "shell.execute_reply": "2024-10-20T13:38:56.817854Z",
     "shell.execute_reply.started": "2024-10-20T13:38:40.526118Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(best_checkpoint), \"Please train the model first to run inference on the finetuned model.\"\n",
    "assert train_instance_segmentation is True, \"Oops. You didn't opt for finetuning using the decoder-based automatic instance segmentation.\"\n",
    "\n",
    "# Let's check the first 5 images. Feel free to comment out the line below to run inference on all images.\n",
    "image_paths = image_paths[:5]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = imageio.imread(image_path)\n",
    "\n",
    "    # Predicted instances\n",
    "    prediction = run_automatic_instance_segmentation(\n",
    "        image=image, checkpoint_path=best_checkpoint, model_type=model_type, device=device\n",
    "    )\n",
    "\n",
    "    # Visualize the predictions\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "\n",
    "    ax[1].imshow(prediction, cmap=get_random_colors(prediction), interpolation=\"nearest\")\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(\"Predictions (AIS)\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f598b",
   "metadata": {},
   "source": [
    "### What next?\n",
    "\n",
    "It's time to get started with your custom finetuned model using the annotator tool. Here is the documentation on how to get started with `µsam`: [Annotation Tools](https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#annotation-tools)\n",
    "\n",
    "Happy annotating!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960393f",
   "metadata": {},
   "source": [
    "*This notebook was last ran on October 20, 2024*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "raysam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.6"
=======
   "version": "3.10.13"
>>>>>>> f86c714eb21114da7975df5f829c3ced9023a37f
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
